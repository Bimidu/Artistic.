{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syntactic & Semantic Model Analysis\n",
    "\n",
    "**Author:** Randil Haturusinghe  \n",
    "**Date:** 2025-11-17  \n",
    "**Project:** ASD Detection System - Artistic\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides comprehensive documentation and analysis of the **Syntactic & Semantic Feature Extraction and Model Training** component of the ASD (Autism Spectrum Disorder) detection system.\n",
    "\n",
    "### Key Components:\n",
    "1. **Feature Extraction**: 27 syntactic and semantic features\n",
    "2. **Preprocessing**: Specialized data cleaning and validation\n",
    "3. **Model Training**: Multiple ML algorithms optimized for syntactic/semantic features\n",
    "4. **Evaluation**: Comprehensive metrics and visualizations\n",
    "\n",
    "### What This Model Does:\n",
    "The syntactic-semantic model analyzes grammatical structures, language complexity, and semantic meaning in conversational transcripts to help identify patterns associated with ASD.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Imports](#setup)\n",
    "2. [Dataset Information](#dataset)\n",
    "3. [Feature Categories](#features)\n",
    "4. [Feature Extraction Process](#extraction)\n",
    "5. [Data Preprocessing](#preprocessing)\n",
    "6. [Model Architecture](#models)\n",
    "7. [Training Configuration](#training)\n",
    "8. [Feature Analysis and Visualizations](#visualizations)\n",
    "9. [Model Performance](#performance)\n",
    "10. [Feature Importance](#importance)\n",
    "11. [Example Usage](#usage)\n",
    "12. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup and Imports <a id=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-17 22:58:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils.logger\u001b[0m:\u001b[36msetup_logger\u001b[0m - \u001b[1mLogger initialized - Level: INFO, File: logs/asd_detection.log\u001b[0m\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "dlopen(/Users/user/PycharmProjects/Artistic./.venv/lib/python3.8/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n  Referenced from: <F47D69E4-1594-3171-ABCC-7156C3E263E1> /Users/user/PycharmProjects/Artistic./.venv/lib/python3.8/site-packages/lightgbm/lib/lib_lightgbm.so\n  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparsers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_parser\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CHATParser\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msyntactic_semantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msyntactic_semantic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SyntacticSemanticFeatures\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msyntactic_semantic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SyntacticSemanticTrainer, SyntacticSemanticPreprocessor\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Set style for visualizations\u001b[39;00m\n",
      "File \u001b[0;32m~/PycharmProjects/Artistic./src/models/__init__.py:16\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mMachine Learning Models Package\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03mAuthor: Bimidu Gunathilake\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_trainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModelTrainer, ModelConfig\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_evaluator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModelEvaluator, EvaluationReport\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_registry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModelRegistry\n",
      "File \u001b[0;32m~/PycharmProjects/Artistic./src/models/model_trainer.py:33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GridSearchCV, RandomizedSearchCV\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightgbm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LGBMClassifier\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_logger\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelpers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m timing_decorator\n",
      "File \u001b[0;32m~/PycharmProjects/Artistic./.venv/lib/python3.8/site-packages/lightgbm/__init__.py:8\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m\"\"\"LightGBM, Light Gradient Boosting Machine.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mContributors: https://github.com/microsoft/LightGBM/graphs/contributors.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbasic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Booster, Dataset, Sequence, register_logger\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallback\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m early_stopping, log_evaluation, record_evaluation, reset_parameter\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CVBooster, cv, train\n",
      "File \u001b[0;32m~/PycharmProjects/Artistic./.venv/lib/python3.8/site-packages/lightgbm/basic.py:221\u001b[0m\n\u001b[1;32m    219\u001b[0m     _LIB \u001b[38;5;241m=\u001b[39m Mock(ctypes\u001b[38;5;241m.\u001b[39mCDLL)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     _LIB \u001b[38;5;241m=\u001b[39m \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m _NUMERIC_TYPES \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m    225\u001b[0m _ArrayLike \u001b[38;5;241m=\u001b[39m Union[List, np\u001b[38;5;241m.\u001b[39mndarray, pd_Series]\n",
      "File \u001b[0;32m~/PycharmProjects/Artistic./.venv/lib/python3.8/site-packages/lightgbm/basic.py:206\u001b[0m, in \u001b[0;36m_load_lib\u001b[0;34m()\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load LightGBM library.\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m lib_path \u001b[38;5;241m=\u001b[39m find_lib_path()\n\u001b[0;32m--> 206\u001b[0m lib \u001b[38;5;241m=\u001b[39m \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcdll\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLoadLibrary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlib_path\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m lib\u001b[38;5;241m.\u001b[39mLGBM_GetLastError\u001b[38;5;241m.\u001b[39mrestype \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_char_p\n\u001b[1;32m    208\u001b[0m callback \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mCFUNCTYPE(\u001b[38;5;28;01mNone\u001b[39;00m, ctypes\u001b[38;5;241m.\u001b[39mc_char_p)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/ctypes/__init__.py:451\u001b[0m, in \u001b[0;36mLibraryLoader.LoadLibrary\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mLoadLibrary\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dlltype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/ctypes/__init__.py:373\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 373\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[0;31mOSError\u001b[0m: dlopen(/Users/user/PycharmProjects/Artistic./.venv/lib/python3.8/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n  Referenced from: <F47D69E4-1594-3171-ABCC-7156C3E263E1> /Users/user/PycharmProjects/Artistic./.venv/lib/python3.8/site-packages/lightgbm/lib/lib_lightgbm.so\n  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().absolute().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# Project imports\n",
    "from src.parsers.chat_parser import CHATParser\n",
    "from src.features.syntactic_semantic.syntactic_semantic import SyntacticSemanticFeatures\n",
    "from src.models.syntactic_semantic import SyntacticSemanticTrainer, SyntacticSemanticPreprocessor\n",
    "from config import config\n",
    "\n",
    "# Set style for visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "print(\"✓ All imports successful\")\n",
    "print(f\"✓ Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Dataset Information <a id=\"dataset\"></a>\n",
    "\n",
    "### Data Source: ASDBank Corpora\n",
    "\n",
    "The model uses conversational transcripts from multiple ASDBank datasets:\n",
    "\n",
    "| Dataset | Description | Format |\n",
    "|---------|-------------|--------|\n",
    "| **asdbank_aac** | AAC device interactions | CHAT (.cha) |\n",
    "| **asdbank_eigsti** | Eigsti lab conversations | CHAT (.cha) |\n",
    "| **asdbank_flusberg** | Flusberg lab data | CHAT (.cha) |\n",
    "| **asdbank_nadig** | Nadig lab transcripts | CHAT (.cha) |\n",
    "| **asdbank_quigley_mcnalley** | Quigley-McNalley corpus | CHAT (.cha) |\n",
    "| **asdbank_rollins** | Rollins lab data | CHAT (.cha) |\n",
    "\n",
    "### CHAT Format\n",
    "CHAT (Codes for the Human Analysis of Transcripts) is a specialized format for transcribing conversational interactions:\n",
    "- Line-by-line utterances with speaker identification\n",
    "- Timing information (when available)\n",
    "- Morphological annotations\n",
    "- Metadata headers with participant information\n",
    "\n",
    "### Diagnosis Labels\n",
    "- **ASD**: Autism Spectrum Disorder\n",
    "- **TD/TYP**: Typically Developing\n",
    "- **DD**: Developmental Delay\n",
    "- **HR/LR**: High Risk / Low Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DATASET CONFIGURATION\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDATASET CONFIGURATION\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m70\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData Directory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mpaths\u001b[38;5;241m.\u001b[39mdata_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAvailable Datasets (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(config\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mdatasets)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(config\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mdatasets, \u001b[38;5;241m1\u001b[39m):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "# Display dataset configuration\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATASET CONFIGURATION\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "print(f\"Data Directory: {config.paths.data_dir}\")\n",
    "print(f\"\\nAvailable Datasets ({len(config.datasets.datasets)}):\")\n",
    "for i, dataset in enumerate(config.datasets.datasets, 1):\n",
    "    print(f\"  {i}. {dataset}\")\n",
    "\n",
    "print(f\"\\nDiagnosis Mapping:\")\n",
    "for code, label in config.datasets.diagnosis_mapping.items():\n",
    "    print(f\"  {code} → {label}\")\n",
    "\n",
    "print(f\"\\nSpeaker Roles:\")\n",
    "for role in config.datasets.speaker_roles:\n",
    "    print(f\"  - {role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Feature Categories <a id=\"features\"></a>\n",
    "\n",
    "The syntactic-semantic model extracts **27 features** across 6 categories:\n",
    "\n",
    "### 3.1 Syntactic Complexity Features (6 features)\n",
    "Analyze grammatical structure complexity:\n",
    "\n",
    "| Feature | Description | Range |\n",
    "|---------|-------------|-------|\n",
    "| `avg_dependency_depth` | Average depth in dependency tree | 0-30 |\n",
    "| `max_dependency_depth` | Maximum dependency tree depth | 0-30 |\n",
    "| `avg_dependency_distance` | Average distance between dependents | 0+ |\n",
    "| `clause_complexity` | Clauses per utterance | 0+ |\n",
    "| `subordination_index` | Subordinate clauses per utterance | 0+ |\n",
    "| `coordination_index` | Coordinated clauses per utterance | 0+ |\n",
    "\n",
    "### 3.2 Grammatical Accuracy Features (5 features)\n",
    "Measure grammatical correctness and consistency:\n",
    "\n",
    "| Feature | Description | Range |\n",
    "|---------|-------------|-------|\n",
    "| `grammatical_error_rate` | Proportion of grammatically incomplete utterances | 0-1 |\n",
    "| `tense_consistency_score` | Consistency in verb tense usage | 0-1 |\n",
    "| `tense_variety` | Variety of tenses used | 0-1 |\n",
    "| `structure_diversity` | Diversity of sentence structures | 0-1 |\n",
    "| `pos_tag_diversity` | Part-of-speech tag diversity | 0-1 |\n",
    "\n",
    "### 3.3 Sentence Structure Features (4 features)\n",
    "Analyze phrase and parse tree characteristics:\n",
    "\n",
    "| Feature | Description | Range |\n",
    "|---------|-------------|-------|\n",
    "| `avg_parse_tree_height` | Average parse tree height | 0-30 |\n",
    "| `noun_phrase_complexity` | Average noun phrase length | 0+ |\n",
    "| `verb_phrase_complexity` | Average verb phrase complexity | 0+ |\n",
    "| `prepositional_phrase_ratio` | Prepositional phrases per utterance | 0+ |\n",
    "\n",
    "### 3.4 Semantic Features (4 features)\n",
    "Measure semantic coherence and meaning:\n",
    "\n",
    "| Feature | Description | Range |\n",
    "|---------|-------------|-------|\n",
    "| `semantic_coherence` | Similarity between consecutive utterances | 0-1 |\n",
    "| `semantic_density` | Content words per utterance | 0+ |\n",
    "| `lexical_diversity_semantic` | Unique content words ratio | 0-1 |\n",
    "| `thematic_consistency` | Repeated content word ratio | 0-1 |\n",
    "\n",
    "### 3.5 Vocabulary Semantic Features (4 features)\n",
    "Analyze vocabulary-level semantic properties:\n",
    "\n",
    "| Feature | Description | Range |\n",
    "|---------|-------------|-------|\n",
    "| `vocabulary_abstractness` | Abstract vs. concrete word ratio | 0-1 |\n",
    "| `semantic_field_diversity` | Diversity of semantic fields | 0-1 |\n",
    "| `word_sense_diversity` | Average word senses per word | 0+ |\n",
    "| `content_word_ratio` | Content vs. function word ratio | 0-1 |\n",
    "\n",
    "### 3.6 Advanced Semantic Features (3 features)\n",
    "Analyze semantic roles and entities:\n",
    "\n",
    "| Feature | Description | Range |\n",
    "|---------|-------------|-------|\n",
    "| `semantic_role_diversity` | Diversity of semantic roles | 0+ |\n",
    "| `entity_density` | Named entities per utterance | 0+ |\n",
    "| `verb_argument_complexity` | Average verb argument count | 0+ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display feature information\n",
    "extractor = SyntacticSemanticFeatures()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SYNTACTIC & SEMANTIC FEATURES\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "print(f\"Total Features: {len(extractor.feature_names)}\\n\")\n",
    "\n",
    "# Group features by category\n",
    "categories = {\n",
    "    'Syntactic Complexity': ['dependency', 'clause', 'subordination', 'coordination'],\n",
    "    'Grammatical': ['grammatical', 'tense', 'pos', 'structure'],\n",
    "    'Sentence Structure': ['parse', 'phrase', 'prepositional'],\n",
    "    'Semantic': ['semantic', 'coherence', 'thematic', 'entity', 'role'],\n",
    "    'Vocabulary': ['vocabulary', 'word', 'lexical', 'content']\n",
    "}\n",
    "\n",
    "for category, keywords in categories.items():\n",
    "    features = [f for f in extractor.feature_names if any(k in f.lower() for k in keywords)]\n",
    "    print(f\"{category} ({len(features)} features):\")\n",
    "    for feature in features:\n",
    "        print(f\"  - {feature}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Feature Extraction Process <a id=\"extraction\"></a>\n",
    "\n",
    "### Extraction Pipeline\n",
    "\n",
    "```\n",
    "CHAT File (.cha)\n",
    "      |\n",
    "      v\n",
    "[1] Parse with CHATParser\n",
    "      |\n",
    "      v\n",
    "[2] Extract child utterances\n",
    "      |\n",
    "      v\n",
    "[3] Process with spaCy NLP\n",
    "      |\n",
    "      v\n",
    "[4] Extract syntactic features\n",
    "      |\n",
    "      v\n",
    "[5] Extract semantic features\n",
    "      |\n",
    "      v\n",
    "[6] Return FeatureResult (27 features)\n",
    "```\n",
    "\n",
    "### NLP Tools Used\n",
    "\n",
    "1. **spaCy** (`en_core_web_sm`):\n",
    "   - POS tagging\n",
    "   - Dependency parsing\n",
    "   - Named entity recognition\n",
    "   - Sentence segmentation\n",
    "\n",
    "2. **NLTK WordNet**:\n",
    "   - Word sense disambiguation\n",
    "   - Semantic field classification\n",
    "   - Abstractness analysis\n",
    "\n",
    "3. **TextStat**:\n",
    "   - Readability metrics (when needed)\n",
    "\n",
    "### Example: Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Extract features from a sample transcript\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE EXTRACTION EXAMPLE\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Find a sample CHAT file\n",
    "data_dir = config.paths.data_dir\n",
    "sample_files = list(data_dir.glob('**/AAC/*.cha'))[:1]\n",
    "\n",
    "if sample_files:\n",
    "    sample_file = sample_files[0]\n",
    "    print(f\"Sample file: {sample_file.name}\")\n",
    "    print(f\"Path: {sample_file}\\n\")\n",
    "    \n",
    "    # Parse transcript\n",
    "    parser = CHATParser()\n",
    "    transcript = parser.parse_file(sample_file)\n",
    "    \n",
    "    print(f\"Participant ID: {transcript.participant_id}\")\n",
    "    print(f\"Total utterances: {transcript.total_utterances}\")\n",
    "    print(f\"Child utterances: {len(transcript.child_utterances)}\\n\")\n",
    "    \n",
    "    # Extract features\n",
    "    extractor = SyntacticSemanticFeatures()\n",
    "    result = extractor.extract(transcript)\n",
    "    \n",
    "    print(f\"Features extracted: {len(result.features)}\")\n",
    "    print(f\"Feature type: {result.feature_type}\")\n",
    "    print(f\"Status: {result.metadata.get('status', 'unknown')}\\n\")\n",
    "    \n",
    "    # Display sample features\n",
    "    print(\"Sample feature values:\")\n",
    "    for i, (name, value) in enumerate(list(result.features.items())[:10], 1):\n",
    "        print(f\"  {i:2d}. {name:<35} = {value:>8.4f}\")\n",
    "    print(\"  ...\")\n",
    "    \n",
    "    # Show sample utterances\n",
    "    print(\"\\nSample child utterances:\")\n",
    "    for i, utt in enumerate(transcript.child_utterances[:3], 1):\n",
    "        print(f\"  {i}. [{utt.speaker}]: {utt.text}\")\n",
    "else:\n",
    "    print(\"No sample files found. Please check data directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Data Preprocessing <a id=\"preprocessing\"></a>\n",
    "\n",
    "### Preprocessing Pipeline\n",
    "\n",
    "The `SyntacticSemanticPreprocessor` applies specialized preprocessing:\n",
    "\n",
    "#### 5.1 Validation\n",
    "- Check for missing values\n",
    "- Validate feature ranges\n",
    "- Verify syntactic feature validity (e.g., dependency depth 0-30)\n",
    "- Verify semantic feature validity (e.g., coherence scores 0-1)\n",
    "\n",
    "#### 5.2 Cleaning\n",
    "- **Dependency features**: Clip to reasonable range [0, 30]\n",
    "- **Complexity features**: Ensure non-negative\n",
    "- **Score features**: Clip to [0, 1] range\n",
    "- **Density features**: Ensure non-negative\n",
    "\n",
    "#### 5.3 Outlier Handling\n",
    "- Method: Clipping (default)\n",
    "- Threshold: 3.5 standard deviations (higher than default for complexity features)\n",
    "\n",
    "#### 5.4 Scaling\n",
    "- Method: StandardScaler (default)\n",
    "- Alternatives: MinMaxScaler, RobustScaler\n",
    "\n",
    "#### 5.5 Feature Selection\n",
    "- Target: 25 features (from 27)\n",
    "- Methods:\n",
    "  - Correlation-based selection\n",
    "  - Variance thresholding\n",
    "  - Recursive feature elimination\n",
    "  - Feature importance\n",
    "\n",
    "### Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display preprocessing configuration\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PREPROCESSING CONFIGURATION\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "preprocessor_config = {\n",
    "    'Target Column': 'diagnosis',\n",
    "    'Test Size': 0.2,\n",
    "    'Random State': 42,\n",
    "    'Handle Complexity Features': True,\n",
    "    'Normalize Dependency Features': True,\n",
    "    'Min Samples': 10,\n",
    "    'Max Missing Ratio': 0.3,\n",
    "    'Missing Strategy': 'median',\n",
    "    'Outlier Method': 'clip',\n",
    "    'Outlier Threshold': 3.5,\n",
    "    'Scaling Method': 'standard',\n",
    "    'Feature Selection': True,\n",
    "    'Number of Features': 25,\n",
    "}\n",
    "\n",
    "for key, value in preprocessor_config.items():\n",
    "    print(f\"{key:.<40} {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VALIDATION CHECKS\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "validation_checks = [\n",
    "    ('Syntactic Features', 'Dependency depth: 0-30, Complexity: non-negative'),\n",
    "    ('Grammatical Features', 'Error rates/scores: 0-1.5, Diversity: 0-1.5'),\n",
    "    ('Semantic Features', 'Coherence: 0-1.5, Density: non-negative'),\n",
    "    ('Vocabulary Features', 'Diversity/ratios: 0-1.5'),\n",
    "]\n",
    "\n",
    "for check_type, check_desc in validation_checks:\n",
    "    print(f\"✓ {check_type}:\")\n",
    "    print(f\"  {check_desc}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Model Architecture <a id=\"models\"></a>\n",
    "\n",
    "### Supported Models\n",
    "\n",
    "The syntactic-semantic trainer supports 7 ML algorithms, optimized for linguistic features:\n",
    "\n",
    "#### 6.1 Random Forest\n",
    "**Type:** Ensemble (bagging)  \n",
    "**Optimized Hyperparameters:**\n",
    "```python\n",
    "{\n",
    "    'n_estimators': 200,      # More trees for complex features\n",
    "    'max_depth': 15,          # Deeper trees for syntactic patterns\n",
    "    'min_samples_split': 3,\n",
    "    'min_samples_leaf': 1,\n",
    "    'n_jobs': -1,\n",
    "}\n",
    "```\n",
    "\n",
    "#### 6.2 XGBoost\n",
    "**Type:** Gradient boosting  \n",
    "**Optimized Hyperparameters:**\n",
    "```python\n",
    "{\n",
    "    'n_estimators': 150,\n",
    "    'max_depth': 8,           # Deeper for syntactic complexity\n",
    "    'learning_rate': 0.1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "}\n",
    "```\n",
    "\n",
    "#### 6.3 LightGBM\n",
    "**Type:** Gradient boosting (efficient)  \n",
    "**Optimized Hyperparameters:**\n",
    "```python\n",
    "{\n",
    "    'n_estimators': 150,\n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "}\n",
    "```\n",
    "\n",
    "#### 6.4 SVM (Support Vector Machine)\n",
    "**Type:** Kernel-based  \n",
    "**Optimized Hyperparameters:**\n",
    "```python\n",
    "{\n",
    "    'C': 1.0,\n",
    "    'kernel': 'rbf',\n",
    "    'gamma': 'scale',\n",
    "}\n",
    "```\n",
    "\n",
    "#### 6.5 Logistic Regression\n",
    "**Type:** Linear classifier  \n",
    "**Optimized Hyperparameters:**\n",
    "```python\n",
    "{\n",
    "    'C': 1.0,\n",
    "    'max_iter': 2000,         # More iterations for convergence\n",
    "    'n_jobs': -1,\n",
    "}\n",
    "```\n",
    "\n",
    "#### 6.6 MLP (Multi-Layer Perceptron)\n",
    "**Type:** Neural network  \n",
    "**Optimized Hyperparameters:**\n",
    "```python\n",
    "{\n",
    "    'hidden_layer_sizes': (150, 100, 50),  # 3 hidden layers\n",
    "    'activation': 'relu',\n",
    "    'max_iter': 1000,\n",
    "    'alpha': 0.001,\n",
    "}\n",
    "```\n",
    "\n",
    "#### 6.7 Gradient Boosting (sklearn)\n",
    "**Type:** Gradient boosting  \n",
    "**Hyperparameters:** Similar to XGBoost\n",
    "\n",
    "### Model Selection Strategy\n",
    "\n",
    "For syntactic-semantic features, we recommend:\n",
    "1. **Primary**: Random Forest (handles non-linear patterns well)\n",
    "2. **Alternative**: XGBoost/LightGBM (good for feature interactions)\n",
    "3. **Baseline**: Logistic Regression (interpretable)\n",
    "\n",
    "### Architecture Diagram\n",
    "\n",
    "```\n",
    "Input: 27 Syntactic/Semantic Features\n",
    "         |\n",
    "         v\n",
    "[Preprocessing Layer]\n",
    "  - Validation\n",
    "  - Cleaning\n",
    "  - Scaling\n",
    "  - Feature Selection → 25 features\n",
    "         |\n",
    "         v\n",
    "[Model Layer]\n",
    "  - Random Forest / XGBoost / etc.\n",
    "         |\n",
    "         v\n",
    "Output: ASD / TD Classification\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model configurations\n",
    "from src.models.syntactic_semantic import SyntacticSemanticTrainer\n",
    "\n",
    "trainer = SyntacticSemanticTrainer()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL CONFIGURATIONS\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "for model_type, params in trainer.SYNTACTIC_SEMANTIC_DEFAULT_PARAMS.items():\n",
    "    print(f\"\\n{model_type.upper().replace('_', ' ')}:\")\n",
    "    print(\"-\" * 50)\n",
    "    for param, value in params.items():\n",
    "        if param != 'random_state':\n",
    "            print(f\"  {param:.<35} {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Training Configuration <a id=\"training\"></a>\n",
    "\n",
    "### Training Process\n",
    "\n",
    "```python\n",
    "# 1. Initialize components\n",
    "preprocessor = SyntacticSemanticPreprocessor()\n",
    "trainer = SyntacticSemanticTrainer()\n",
    "\n",
    "# 2. Preprocess data\n",
    "X_train, X_test, y_train, y_test = preprocessor.fit_transform(df)\n",
    "\n",
    "# 3. Train models\n",
    "results = trainer.train_multiple_models(\n",
    "    X_train, y_train, X_test, y_test\n",
    ")\n",
    "\n",
    "# 4. Evaluate and compare\n",
    "best_model = results['best_model']\n",
    "```\n",
    "\n",
    "### Training Parameters\n",
    "\n",
    "| Parameter | Value | Description |\n",
    "|-----------|-------|-------------|\n",
    "| Train/Test Split | 80/20 | Stratified split |\n",
    "| Cross-Validation | 5-fold | For hyperparameter tuning |\n",
    "| Scoring Metric | F1 (weighted) | Handles class imbalance |\n",
    "| Random State | 42 | Reproducibility |\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "The model is evaluated using:\n",
    "- **Accuracy**: Overall classification accuracy\n",
    "- **Precision**: Positive predictive value\n",
    "- **Recall**: Sensitivity\n",
    "- **F1 Score**: Harmonic mean of precision and recall\n",
    "- **ROC-AUC**: Area under ROC curve\n",
    "- **Confusion Matrix**: Classification breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING CONFIGURATION SUMMARY\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "training_config = {\n",
    "    'Data Split': '80% train, 20% test',\n",
    "    'Split Strategy': 'Stratified (preserves class distribution)',\n",
    "    'Cross-Validation': '5-fold CV for hyperparameter tuning',\n",
    "    'Primary Metric': 'F1 Score (weighted)',\n",
    "    'Secondary Metrics': 'Accuracy, Precision, Recall, ROC-AUC',\n",
    "    'Random State': 42,\n",
    "    'Parallel Jobs': -1,\n",
    "}\n",
    "\n",
    "for key, value in training_config.items():\n",
    "    print(f\"{key:.<40} {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODELS TRAINED\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "models_info = [\n",
    "    ('Random Forest', 'Ensemble method, handles non-linear patterns'),\n",
    "    ('XGBoost', 'Gradient boosting, excellent for feature interactions'),\n",
    "    ('LightGBM', 'Fast gradient boosting, memory efficient'),\n",
    "    ('SVM', 'Kernel-based, good for high-dimensional data'),\n",
    "    ('Logistic Regression', 'Linear baseline, highly interpretable'),\n",
    "]\n",
    "\n",
    "for i, (model_name, description) in enumerate(models_info, 1):\n",
    "    print(f\"{i}. {model_name}\")\n",
    "    print(f\"   → {description}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Feature Analysis and Visualizations <a id=\"visualizations\"></a>\n",
    "\n",
    "This section demonstrates feature distributions and relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic data for visualization demonstration\n",
    "# (Replace with actual data when available)\n",
    "\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "\n",
    "# Generate synthetic feature data\n",
    "feature_data = {\n",
    "    'avg_dependency_depth': np.random.normal(3.5, 1.2, n_samples).clip(0, 10),\n",
    "    'max_dependency_depth': np.random.normal(6.0, 2.0, n_samples).clip(0, 15),\n",
    "    'clause_complexity': np.random.normal(1.5, 0.8, n_samples).clip(0, 5),\n",
    "    'subordination_index': np.random.normal(0.8, 0.5, n_samples).clip(0, 3),\n",
    "    'grammatical_error_rate': np.random.beta(2, 5, n_samples),\n",
    "    'tense_consistency_score': np.random.beta(8, 2, n_samples),\n",
    "    'semantic_coherence': np.random.beta(6, 3, n_samples),\n",
    "    'semantic_density': np.random.normal(4.0, 1.5, n_samples).clip(0, 10),\n",
    "    'vocabulary_abstractness': np.random.beta(3, 4, n_samples),\n",
    "    'content_word_ratio': np.random.beta(5, 3, n_samples),\n",
    "}\n",
    "\n",
    "# Add diagnosis labels\n",
    "feature_data['diagnosis'] = np.random.choice(['ASD', 'TD'], n_samples, p=[0.4, 0.6])\n",
    "\n",
    "df_demo = pd.DataFrame(feature_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAMPLE DATA GENERATED FOR VISUALIZATION\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "print(f\"Shape: {df_demo.shape}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df_demo['diagnosis'].value_counts())\n",
    "print(\"\\nSample statistics:\")\n",
    "print(df_demo.describe().round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Feature Distribution by Diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature distribution plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Syntactic & Semantic Feature Distributions by Diagnosis', fontsize=16, fontweight='bold')\n",
    "\n",
    "features_to_plot = [\n",
    "    'avg_dependency_depth',\n",
    "    'semantic_coherence',\n",
    "    'grammatical_error_rate',\n",
    "    'vocabulary_abstractness'\n",
    "]\n",
    "\n",
    "for idx, feature in enumerate(features_to_plot):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    # Violin plot\n",
    "    sns.violinplot(data=df_demo, x='diagnosis', y=feature, ax=ax, palette='Set2')\n",
    "    ax.set_title(f'{feature.replace(\"_\", \" \").title()}', fontweight='bold')\n",
    "    ax.set_xlabel('Diagnosis')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Feature distribution plots generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Feature Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Select numeric features\n",
    "numeric_features = df_demo.select_dtypes(include=[np.number]).columns\n",
    "correlation_matrix = df_demo[numeric_features].corr()\n",
    "\n",
    "# Create heatmap\n",
    "sns.heatmap(\n",
    "    correlation_matrix,\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    cmap='coolwarm',\n",
    "    center=0,\n",
    "    square=True,\n",
    "    linewidths=1,\n",
    "    cbar_kws={'shrink': 0.8}\n",
    ")\n",
    "\n",
    "plt.title('Feature Correlation Matrix', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Correlation heatmap generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Feature Category Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle('Feature Category Comparisons: ASD vs TD', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Syntactic features\n",
    "syntactic_features = ['avg_dependency_depth', 'clause_complexity', 'subordination_index']\n",
    "df_syntactic = df_demo.groupby('diagnosis')[syntactic_features].mean()\n",
    "df_syntactic.T.plot(kind='bar', ax=axes[0], color=['#ff7f0e', '#1f77b4'])\n",
    "axes[0].set_title('Syntactic Complexity', fontweight='bold')\n",
    "axes[0].set_ylabel('Average Value')\n",
    "axes[0].set_xlabel('Feature')\n",
    "axes[0].legend(title='Diagnosis')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "plt.setp(axes[0].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Grammatical features\n",
    "grammatical_features = ['grammatical_error_rate', 'tense_consistency_score']\n",
    "df_grammatical = df_demo.groupby('diagnosis')[grammatical_features].mean()\n",
    "df_grammatical.T.plot(kind='bar', ax=axes[1], color=['#ff7f0e', '#1f77b4'])\n",
    "axes[1].set_title('Grammatical Features', fontweight='bold')\n",
    "axes[1].set_ylabel('Average Value')\n",
    "axes[1].set_xlabel('Feature')\n",
    "axes[1].legend(title='Diagnosis')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "plt.setp(axes[1].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Semantic features\n",
    "semantic_features = ['semantic_coherence', 'semantic_density', 'vocabulary_abstractness']\n",
    "df_semantic = df_demo.groupby('diagnosis')[semantic_features].mean()\n",
    "df_semantic.T.plot(kind='bar', ax=axes[2], color=['#ff7f0e', '#1f77b4'])\n",
    "axes[2].set_title('Semantic Features', fontweight='bold')\n",
    "axes[2].set_ylabel('Average Value')\n",
    "axes[2].set_xlabel('Feature')\n",
    "axes[2].legend(title='Diagnosis')\n",
    "axes[2].grid(True, alpha=0.3, axis='y')\n",
    "plt.setp(axes[2].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Category comparison plots generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Feature Value Ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots showing feature ranges\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "# Prepare data for box plot\n",
    "features_for_box = [\n",
    "    'avg_dependency_depth', 'clause_complexity', 'subordination_index',\n",
    "    'grammatical_error_rate', 'semantic_coherence', 'vocabulary_abstractness'\n",
    "]\n",
    "\n",
    "df_box = df_demo[features_for_box]\n",
    "df_box_normalized = (df_box - df_box.min()) / (df_box.max() - df_box.min())\n",
    "\n",
    "# Create box plot\n",
    "bp = ax.boxplot(\n",
    "    [df_box_normalized[col].values for col in df_box_normalized.columns],\n",
    "    labels=[col.replace('_', '\\n') for col in df_box_normalized.columns],\n",
    "    patch_artist=True,\n",
    "    showmeans=True\n",
    ")\n",
    "\n",
    "# Color boxes\n",
    "colors = ['lightblue', 'lightgreen', 'lightyellow', 'lightcoral', 'lightpink', 'plum']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "ax.set_title('Feature Value Ranges (Normalized)', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Normalized Value (0-1)')\n",
    "ax.set_xlabel('Features')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Feature range box plots generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Model Performance <a id=\"performance\"></a>\n",
    "\n",
    "### Performance Metrics\n",
    "\n",
    "This section would show actual model performance once trained on real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated model performance comparison\n",
    "# (Replace with actual results when training on real data)\n",
    "\n",
    "performance_data = {\n",
    "    'Model': ['Random Forest', 'XGBoost', 'LightGBM', 'SVM', 'Logistic Regression'],\n",
    "    'Accuracy': [0.82, 0.80, 0.81, 0.78, 0.75],\n",
    "    'Precision': [0.81, 0.79, 0.80, 0.77, 0.74],\n",
    "    'Recall': [0.83, 0.81, 0.82, 0.79, 0.76],\n",
    "    'F1 Score': [0.82, 0.80, 0.81, 0.78, 0.75],\n",
    "    'ROC-AUC': [0.88, 0.86, 0.87, 0.84, 0.81],\n",
    "}\n",
    "\n",
    "df_performance = pd.DataFrame(performance_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL PERFORMANCE COMPARISON (Simulated)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "print(df_performance.to_string(index=False))\n",
    "\n",
    "# Performance comparison plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(df_performance['Model']))\n",
    "width = 0.15\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC-AUC']\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "\n",
    "for i, (metric, color) in enumerate(zip(metrics, colors)):\n",
    "    offset = width * (i - 2)\n",
    "    ax.bar(x + offset, df_performance[metric], width, label=metric, color=color, alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Model', fontweight='bold')\n",
    "ax.set_ylabel('Score', fontweight='bold')\n",
    "ax.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(df_performance['Model'], rotation=15, ha='right')\n",
    "ax.legend(loc='lower right')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.set_ylim([0.7, 0.95])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Performance comparison plot generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated confusion matrix for best model (Random Forest)\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Simulated predictions\n",
    "y_true = np.random.choice(['ASD', 'TD'], 100, p=[0.4, 0.6])\n",
    "y_pred = y_true.copy()\n",
    "# Add some errors\n",
    "error_indices = np.random.choice(100, 18, replace=False)\n",
    "for idx in error_indices:\n",
    "    y_pred[idx] = 'TD' if y_pred[idx] == 'ASD' else 'ASD'\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=['ASD', 'TD'])\n",
    "\n",
    "# Plot confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['ASD', 'TD'])\n",
    "disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
    "ax.set_title('Confusion Matrix - Random Forest (Simulated)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Confusion matrix plot generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Feature Importance <a id=\"importance\"></a>\n",
    "\n",
    "### Most Important Features for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated feature importance\n",
    "# (Replace with actual feature importance from trained model)\n",
    "\n",
    "feature_importance_data = {\n",
    "    'Feature': [\n",
    "        'semantic_coherence',\n",
    "        'avg_dependency_depth',\n",
    "        'grammatical_error_rate',\n",
    "        'tense_consistency_score',\n",
    "        'vocabulary_abstractness',\n",
    "        'semantic_density',\n",
    "        'clause_complexity',\n",
    "        'subordination_index',\n",
    "        'content_word_ratio',\n",
    "        'lexical_diversity_semantic',\n",
    "    ],\n",
    "    'Importance': [0.145, 0.132, 0.118, 0.105, 0.095, 0.088, 0.075, 0.068, 0.062, 0.055],\n",
    "    'Category': [\n",
    "        'Semantic', 'Syntactic', 'Grammatical', 'Grammatical', 'Vocabulary',\n",
    "        'Semantic', 'Syntactic', 'Syntactic', 'Vocabulary', 'Semantic'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_importance = pd.DataFrame(feature_importance_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TOP 10 MOST IMPORTANT FEATURES (Simulated)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "print(df_importance.to_string(index=False))\n",
    "\n",
    "# Feature importance plot\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Color by category\n",
    "category_colors = {\n",
    "    'Syntactic': '#1f77b4',\n",
    "    'Grammatical': '#ff7f0e',\n",
    "    'Semantic': '#2ca02c',\n",
    "    'Vocabulary': '#d62728'\n",
    "}\n",
    "\n",
    "colors = [category_colors[cat] for cat in df_importance['Category']]\n",
    "\n",
    "bars = ax.barh(\n",
    "    df_importance['Feature'],\n",
    "    df_importance['Importance'],\n",
    "    color=colors,\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "ax.set_xlabel('Importance Score', fontweight='bold')\n",
    "ax.set_ylabel('Feature', fontweight='bold')\n",
    "ax.set_title('Top 10 Feature Importance (Random Forest)', fontsize=14, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=color, alpha=0.7, label=cat) \n",
    "                   for cat, color in category_colors.items()]\n",
    "ax.legend(handles=legend_elements, loc='lower right', title='Category')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, value) in enumerate(zip(bars, df_importance['Importance'])):\n",
    "    ax.text(value + 0.003, bar.get_y() + bar.get_height()/2, \n",
    "            f'{value:.3f}', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Feature importance plot generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category-wise importance\n",
    "category_importance = df_importance.groupby('Category')['Importance'].sum().sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "colors_cat = [category_colors[cat] for cat in category_importance.index]\n",
    "bars = ax.bar(\n",
    "    category_importance.index,\n",
    "    category_importance.values,\n",
    "    color=colors_cat,\n",
    "    alpha=0.7,\n",
    "    edgecolor='black',\n",
    "    linewidth=1.5\n",
    ")\n",
    "\n",
    "ax.set_ylabel('Total Importance Score', fontweight='bold')\n",
    "ax.set_xlabel('Feature Category', fontweight='bold')\n",
    "ax.set_title('Feature Importance by Category', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, category_importance.values):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "            f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE CATEGORY IMPORTANCE\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "for cat, imp in category_importance.items():\n",
    "    print(f\"{cat:.<30} {imp:.4f}\")\n",
    "\n",
    "print(\"\\n✓ Category importance plot generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Example Usage <a id=\"usage\"></a>\n",
    "\n",
    "### Complete Pipeline Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "# ========================================================================\n",
    "# COMPLETE PIPELINE EXAMPLE\n",
    "# ========================================================================\n",
    "\n",
    "# Step 1: Parse CHAT transcript\n",
    "from src.parsers.chat_parser import CHATParser\n",
    "\n",
    "parser = CHATParser()\n",
    "transcript = parser.parse_file('path/to/transcript.cha')\n",
    "\n",
    "# Step 2: Extract syntactic/semantic features\n",
    "from src.features.syntactic_semantic import SyntacticSemanticFeatures\n",
    "\n",
    "extractor = SyntacticSemanticFeatures()\n",
    "features = extractor.extract(transcript)\n",
    "\n",
    "print(f\"Extracted {len(features.features)} features\")\n",
    "# Output: Extracted 27 features\n",
    "\n",
    "# Step 3: Process multiple transcripts and create DataFrame\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path('data/asdbank_aac/AAC')\n",
    "all_features = []\n",
    "\n",
    "for file_path in data_dir.glob('*.cha'):\n",
    "    transcript = parser.parse_file(file_path)\n",
    "    result = extractor.extract(transcript)\n",
    "    \n",
    "    # Add metadata\n",
    "    feature_dict = result.features.copy()\n",
    "    feature_dict['participant_id'] = transcript.participant_id\n",
    "    feature_dict['diagnosis'] = 'ASD'  # Or from metadata\n",
    "    \n",
    "    all_features.append(feature_dict)\n",
    "\n",
    "df = pd.DataFrame(all_features)\n",
    "\n",
    "# Step 4: Preprocess data\n",
    "from src.models.syntactic_semantic import SyntacticSemanticPreprocessor\n",
    "\n",
    "preprocessor = SyntacticSemanticPreprocessor(\n",
    "    target_column='diagnosis',\n",
    "    test_size=0.2,\n",
    "    feature_selection=True,\n",
    "    n_features=25\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = preprocessor.fit_transform(df)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "# Output: Training set: (80, 25)\n",
    "#         Test set: (20, 25)\n",
    "\n",
    "# Step 5: Train models\n",
    "from src.models.syntactic_semantic import SyntacticSemanticTrainer\n",
    "\n",
    "trainer = SyntacticSemanticTrainer()\n",
    "\n",
    "results = trainer.train_multiple_models(\n",
    "    X_train, y_train,\n",
    "    X_test, y_test\n",
    ")\n",
    "\n",
    "# Step 6: Evaluate and select best model\n",
    "best_model_name = results['best_model']\n",
    "best_model = results['models'][best_model_name]\n",
    "\n",
    "print(f\"\\nBest model: {best_model_name}\")\n",
    "print(f\"Performance: {results['evaluation_summary'][best_model_name]}\")\n",
    "\n",
    "# Step 7: Get feature importance\n",
    "importance_df = trainer.get_syntactic_semantic_feature_importance(\n",
    "    best_model_name,\n",
    "    X_train.columns.tolist(),\n",
    "    top_n=10\n",
    ")\n",
    "\n",
    "print(\"\\nTop 10 important features:\")\n",
    "print(importance_df)\n",
    "\n",
    "# Step 8: Save model and preprocessor\n",
    "trainer.save_model(\n",
    "    best_model_name,\n",
    "    'models/syntactic_semantic_best_model.pkl'\n",
    ")\n",
    "\n",
    "preprocessor.save('models/syntactic_semantic_preprocessor.pkl')\n",
    "\n",
    "print(\"\\n✓ Model and preprocessor saved\")\n",
    "\n",
    "# Step 9: Load and use for prediction\n",
    "# Load saved preprocessor and model\n",
    "preprocessor_loaded = SyntacticSemanticPreprocessor.load(\n",
    "    'models/syntactic_semantic_preprocessor.pkl'\n",
    ")\n",
    "\n",
    "# Make predictions on new data\n",
    "new_transcript = parser.parse_file('path/to/new_transcript.cha')\n",
    "new_features = extractor.extract(new_transcript)\n",
    "\n",
    "# Preprocess and predict\n",
    "# ... (additional preprocessing steps)\n",
    "prediction = best_model.predict(new_features_processed)\n",
    "probability = best_model.predict_proba(new_features_processed)\n",
    "\n",
    "print(f\"\\nPrediction: {prediction[0]}\")\n",
    "print(f\"Probability: ASD={probability[0][0]:.2f}, TD={probability[0][1]:.2f}\")\n",
    "\n",
    "# ========================================================================\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Conclusion <a id=\"conclusion\"></a>\n",
    "\n",
    "### Summary\n",
    "\n",
    "This notebook documented the **Syntactic & Semantic Model** component of the ASD detection system:\n",
    "\n",
    "#### Key Achievements:\n",
    "1. **27 Features** extracted across 6 categories:\n",
    "   - Syntactic Complexity (6 features)\n",
    "   - Grammatical Accuracy (5 features)\n",
    "   - Sentence Structure (4 features)\n",
    "   - Semantic Features (4 features)\n",
    "   - Vocabulary Semantic (4 features)\n",
    "   - Advanced Semantic (3 features)\n",
    "\n",
    "2. **Specialized Preprocessing**:\n",
    "   - Syntactic/semantic-specific validation\n",
    "   - Feature range normalization\n",
    "   - Outlier handling with higher tolerance\n",
    "   - Feature selection (27 → 25 features)\n",
    "\n",
    "3. **Multiple ML Models**:\n",
    "   - Random Forest (best performance)\n",
    "   - XGBoost / LightGBM\n",
    "   - SVM\n",
    "   - Logistic Regression\n",
    "   - MLP Neural Network\n",
    "\n",
    "4. **Comprehensive Evaluation**:\n",
    "   - Accuracy, Precision, Recall, F1, ROC-AUC\n",
    "   - Feature importance analysis\n",
    "   - Category-wise performance\n",
    "\n",
    "### Implementation Status\n",
    "\n",
    "**Status:** ✓ FULLY IMPLEMENTED  \n",
    "**Author:** Randil Haturusinghe  \n",
    "**Date:** 2025-11-17\n",
    "\n",
    "All components are production-ready:\n",
    "- ✓ Feature extraction\n",
    "- ✓ Preprocessing\n",
    "- ✓ Model training\n",
    "- ✓ Evaluation\n",
    "- ✓ Visualization\n",
    "\n",
    "### Future Enhancements\n",
    "\n",
    "Potential improvements:\n",
    "1. Add more advanced semantic features (e.g., topic modeling)\n",
    "2. Implement ensemble methods combining multiple models\n",
    "3. Add cross-dataset validation\n",
    "4. Implement attention mechanisms for interpretability\n",
    "5. Add real-time feature extraction capabilities\n",
    "\n",
    "### References\n",
    "\n",
    "- **spaCy**: Industrial-strength NLP - https://spacy.io/\n",
    "- **NLTK**: Natural Language Toolkit - https://www.nltk.org/\n",
    "- **WordNet**: Lexical database - https://wordnet.princeton.edu/\n",
    "- **CHAT Format**: CHILDES - https://talkbank.org/\n",
    "\n",
    "### Contact\n",
    "\n",
    "For questions or issues related to the syntactic-semantic model:\n",
    "- Author: Randil Haturusinghe\n",
    "- Project: Artistic - ASD Detection System\n",
    "- Repository: https://github.com/Bimidu/Artistic\n",
    "\n",
    "---\n",
    "\n",
    "**End of Notebook**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
